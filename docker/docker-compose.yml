version: "3.8"

services:
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: chatbot_postgres
    environment:
      POSTGRES_DB: chatbot_db
      POSTGRES_USER: chatbot_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-chatbot_pass_2024}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U chatbot_user -d chatbot_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: chatbot_qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    healthcheck:
      disable: true

  # Main Application
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: chatbot_app
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_started
    env_file:
      - ../.env
    environment:
      # Database
      DATABASE_URL: postgresql://chatbot_user:${POSTGRES_PASSWORD:-chatbot_pass_2024}@postgres:5432/chatbot_db
      
      # Qdrant
      QDRANT_URL: http://qdrant:6333
      QDRANT_COLLECTION: documents
      
      # Embedding Model
      MODEL_NAME: sentence-transformers/all-MiniLM-L6-v2
      MODEL_DIR: /app/models
      HF_HOME: /hf_cache
      
      # LLM Model (GGUF)
      LLM_MODEL: ${LLM_MODEL:-qwen2.5-3b-q4}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE:-0.7}
      LLM_MAX_OUTPUT_TOKENS: ${LLM_MAX_OUTPUT_TOKENS:-1024}
      
      # llama.cpp optimization
      LLAMA_THREADS: ${LLAMA_THREADS:-4}
      
      # Python
      PYTHONPATH: /app
      PYTHONUNBUFFERED: "1"
      
    volumes:
      - ../src:/app/src
      - ../database:/app/database
      - ../auth:/app/auth
      - ../services:/app/services
      - ../components:/app/components
      - ../models:/app/models
      - ../data:/app/data
      - ../logs:/app/logs
      - hf_cache:/hf_cache
      # IMPORTANT: Mount GGUF models để persistent
      - gguf_models:/app/models/gguf
    command: >
      sh -c "streamlit run src/app_v2.py
             --server.port=8501
             --server.address=0.0.0.0
             --server.headless=true
             --server.runOnSave=true"
    ports:
      - "8501:8501"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8501/_stcore/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    # Optional: Giới hạn resources
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '4'
    #       memory: 8G

volumes:
  postgres_data:
  qdrant_data:
  hf_cache:
  gguf_models:  # NEW: Volume cho GGUF models